{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sree Ganeshaaya Namaha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS Notebook  Gives a Glimpse of how we can use NLP on Network Events.\n",
    "Natural language processing (NLP): is a subfield of artificial intelligence, used for text/speech processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Achieve following here by using NLP Techniques.\n",
    "  1) Word2Vec :  Convert each word in Event messages into numerical vectors. Vectors are formed based on the relationship between adjecent words.\n",
    "  <br>2) Group the similar events using Latent Semantic Analysis(LSA) - uses Singular Value Decomposition(SVD).\n",
    "  <br>3) Create top event list for each device - using the TFIDF score. TFIDF is a way of measuring the weight of a word in the text corpus. - this can be achieved using a sql query too, but query will hang as the number of events grow.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "# Use tqdm to show progress of an pandas function we use\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize function - Splits each event message into an array of words.\n",
    "replace \".\" with \"_\" ,  As  \".\" is considered as a separator when we use gensim word2vec.\n",
    "Also, let us eliminate the following - \n",
    " 1. numbers\n",
    " 2. word containing single letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "    \"\"\"\n",
    "    :params sentence_list: list of strings\n",
    "    :returns tok_sentences: list of list of tokens\n",
    "    \"\"\"\n",
    "    tok_sentences = []\n",
    "    for sent in sentences:\n",
    "        sent=sent.lower()\n",
    "        #sent=re.sub(\"\\d+\\.\\d+\\.\\d+\\.\\d+\",ipv4_repl,sent)\n",
    "        sent=re.sub(\"\\.\",\"_\",sent)\n",
    "        toks=sent.split(\" \")\n",
    "        revised=[]\n",
    "        for tok in toks:\n",
    "            tok=tok.strip()\n",
    "            if str(tok).isnumeric() == False and len(tok)>1:\n",
    "                revised.append(tok)\n",
    "            \n",
    "        tok_sentences.append(revised)\n",
    "    return tok_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the .csv containing Event Messages.\n",
    "we use 'HOST', 'NORM_MSG' fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Index(['TIMESTAMP', 'HOST', 'NORM_MSG', 'EPOCH'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#df=pd.read_csv(\"faults/isat/isat_filtered_1.csv\")\n",
    "df=pd.read_csv(\"input/events.csv\")\n",
    "print(df[\"HOST\"].size, df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us concatenate the host with the message and tokenize each into array of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate device with message\n",
    "msgs=[]\n",
    "for i,row in df.iterrows():    \n",
    "    #msgs.append(\"{} {}\".format(row['HOST'],row['EMESSAGE']))\n",
    "    msgs.append(\"{} {}\".format(row['HOST'],row['NORM_MSG']))\n",
    "                \n",
    "tokens=tokenize(msgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec:\n",
    "# Each word in the Event Corpus will be converted to a numeric vector here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words= 95\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=10\n",
    "w2c = Word2Vec(tokens, size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "\n",
    "tmparr=w2c.wv.index2word\n",
    "word2index={}\n",
    "index2word={}\n",
    "for i,w in enumerate(tmparr):\n",
    "    word2index[w]=i\n",
    "    index2word[i]=w\n",
    "num_words=len(word2index)\n",
    "\n",
    "print(\"num_words=\",num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the numeric vector generated for a sample word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%bgp-3-notification:  is represented in vector form as : [-0.03698418  0.04435233  0.0117104   0.01732238  0.04707624  0.04330237\n",
      " -0.02627918  0.02136641  0.02805902 -0.03972169]\n",
      "\n",
      "Closest words to  %bgp-3-notification:  are : [('%l2-bm-6-active', 0.5415895581245422), ('high', 0.43139877915382385), ('4:', 0.4216136932373047), ('reachable', 0.4019451439380646), ('interface11', 0.39195916056632996), ('hmac', 0.31841742992401123), ('ifmgr', 0.3095794916152954), ('(not', 0.29967406392097473), ('from', 0.26347410678863525), ('%xxxx-3-platform:', 0.25406414270401)]\n"
     ]
    }
   ],
   "source": [
    "#w=\"bgp\"\n",
    "w=\"%bgp-3-notification:\"\n",
    "i=word2index[w]\n",
    "print (w, \" is represented in vector form as :\", w2c.wv[w])\n",
    "print(\"\\nClosest words to \", w, \" are :\", w2c.wv.similar_by_word(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent semantic analysis (LSA) - Group Similar Events using LSA\n",
    "is a technique in natural language processing, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
    "\n",
    "LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). \n",
    "\n",
    "A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. \n",
    "\n",
    "Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us group events in to two broad classes using LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis using Python\n",
    "\n",
    "data=[\" \".join(sent_tokens)  for sent_tokens in tokens]\n",
    "\n",
    "# Creating Tfidf Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "# Visualizing the Tfidf Model\n",
    "#print(X[0])\n",
    "\n",
    "# Creating the SVD\n",
    "lsa = TruncatedSVD(n_components = 2)#, n_iter = 5 by default.\n",
    "lsa.fit(X)\n",
    "\n",
    "\n",
    "# First Column of V\n",
    "#row1 = lsa.components_[3]\n",
    "\n",
    "\n",
    "concept_words={}#{group 0: [(word1:score1),(word2,score2)]}\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "    #let us consider topmost 1000 terms only.\n",
    "    #sortedTerms = sortedTerms[:1000]\n",
    "    term_and_score={}\n",
    "    for t in sortedTerms:\n",
    "        if t[1] < 0:\n",
    "            break\n",
    "        term_and_score[t[0]]=t[1]\n",
    "    \n",
    "    concept_words[\"Group \"+str(i)] = term_and_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data structure containing sentence and its score for each group - {group:[{sentence:score}, ...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "group_and_sent_scores={}#{group:[{sentence:score}, ...]}\n",
    "for group,term_and_score in concept_words.items():\n",
    "    \n",
    "    sentence_scores={}\n",
    "    for i,words in enumerate(tokens):\n",
    "        score = 0\n",
    "        sent=\" \".join(words)\n",
    "        if sent not in sentence_scores:\n",
    "            for word in words:\n",
    "                if word in term_and_score:\n",
    "                    score += term_and_score[word]\n",
    "            sentence_scores[sent]=score\n",
    "    group_and_sent_scores[group]=sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a datastructure containing sentence and its score for each group/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups= dict_keys(['Group 0', 'Group 1'])\n"
     ]
    }
   ],
   "source": [
    "print (\"Total groups=\",group_and_sent_scores.keys())\n",
    "\n",
    " \n",
    "#create datastrcture\n",
    "#sentences_and_group_scores : {sent:{\"group1\":score1,\"group2:\",score2..},..}\n",
    "#ex. -{ rmf_svr %ha-redcon-1-standby_not_ready standby card is not ready\n",
    "# {'Group 6': 0.47417499636854393, 'Group 9': 0.27952190650844383, 'Group 3': 0.19261210099664636..}..}\n",
    "\n",
    "sentences_and_group_scores={}\n",
    "for key,sentence_and_scores in group_and_sent_scores.items():\n",
    "    grp=key\n",
    "    for sent,score in sentence_and_scores.items():\n",
    "        if sent not in sentences_and_group_scores:\n",
    "            sentences_and_group_scores[sent]={}\n",
    "        sentences_and_group_scores[sent][grp]=score\n",
    "    \n",
    "\n",
    "group_and_sentences_and_scores={}\n",
    "for sentence,grp_and_scores in sentences_and_group_scores.items():\n",
    "    #sort by scores\n",
    "    sorted_scores=sorted(grp_and_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    #the top group for this sentence\n",
    "    top_grp_and_score_for_me=sorted_scores[0]\n",
    "    grp=top_grp_and_score_for_me[0]\n",
    "    score=top_grp_and_score_for_me[1]\n",
    "    if grp not in group_and_sentences_and_scores:\n",
    "        group_and_sentences_and_scores[grp]=[]\n",
    "    group_and_sentences_and_scores[grp].append((sentence,score))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the top five events belonging to each group.\n",
    "Note: As we have very few number of events in the sample file - following output groups the events into only one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Group 0 :\n",
      "device3 %bgp-4-vpn_nh_if: nexthop device3 may not be reachable from neigbor device241 not loopback 2.7559453136850975\n",
      "device1 bm-distrib %l2-bm-6-active interface4 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold) 1.5474842393805452\n",
      "device1 bm-distrib %l2-bm-6-active interface7 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold) 1.5282141346112346\n",
      "device1 bm-distrib %l2-bm-6-active interface3 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold) 1.5282141345355682\n",
      "device1 bm-distrib %l2-bm-6-active interface10 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold) 1.5282141345311626\n"
     ]
    }
   ],
   "source": [
    "#sort the sentences in reverse order for each group\n",
    "for grp, sents_and_scores in group_and_sentences_and_scores.items():\n",
    "    sorted_sents=sorted(sents_and_scores, key=lambda x: x[1], reverse=True)\n",
    "    group_and_sentences_and_scores[grp]=sorted_sents\n",
    "    print (\"\\n\", grp, \":\")\n",
    "    for sent,score in sorted_sents[0:5]:\n",
    "        print (sent, score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Summary for each device using NLP\n",
    "Event summary is nothing but top occurring events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "\n",
      " device1 :\n",
      "device1 interface1 bfd_agent %l2-bfd-6-adjacency_delete adjacency to neighbor device66 on interface bundle2 was deleted\n",
      "device1 bm-distrib %l2-bm-6-active interface4 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface11 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface5 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface3 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface10 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface7 is no longer active as part of bundle1 (not enough links available to meet minimum-active threshold)\n",
      "device1 bm-distrib %l2-bm-6-active interface6 is no longer active as part of bundle1 (link is down)\n",
      "device1 ifmgr %pkt_infra-link-3-updown interface bundle1 changed state to down\n",
      "device1 ifmgr %pkt_infra-link-3-updown interface interface6 changed state to down\n",
      "---------------------------------------------------------\n",
      "\n",
      " device2 :\n",
      "device2 interface1 bfd_agent %l2-bfd-6-adjacency_delete adjacency to neighbor device67 on interface bundle2 was deleted\n",
      "device2 bm-distrib %l2-bm-6-active interface4 is no longer active as part of bundle1 (link is down)\n",
      "device2 bm-distrib %l2-bm-6-active :interface13 is no longer active as part of bundle1 (link is down)\n",
      "device2 ifmgr %pkt_infra-link-3-updown interface interface4 changed state to down\n",
      "device2 ifmgr %pkt_infra-link-3-updown interface interface12 changed state to down\n",
      "device2 ifmgr %pkt_infra-link-3-updown interfaceinterface13 changed state to down\n",
      "---------------------------------------------------------\n",
      "\n",
      " device3 :\n",
      "device3 %bgp-4-vpn_nh_if: nexthop device3 may not be reachable from neigbor device241 not loopback\n",
      "device3 %bgp-3-notification: sent to neighbor device98/0 (hold time expired) bytes\n",
      "device3 %xxxx-3-platform: sip0: cpp_cp: qfp:00 thread:108 ts:00071556161958872244 %yyyy_error: ipsec sa receives hmac error dp handle\n",
      "---------------------------------------------------------\n",
      "\n",
      " device4 :\n",
      "device4 %iftmc-slot4-2-iftmc_res_alloc_warn: iftmc resource allocation warning: high resource usage on asic 4: percentage of ldb is being used\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "# Word counts \n",
    "word2count = {}\n",
    "for sent in tokens:\n",
    "    for word in sent:# nltk.word_tokenize(clean_text):\n",
    "        if word not in word2count:\n",
    "            word2count[word]=0\n",
    "        word2count[word] += 1\n",
    "\n",
    "# Converting counts to weights\n",
    "max_count = max(word2count.values())\n",
    "for key in word2count.keys():\n",
    "    word2count[key] = word2count[key]/max_count\n",
    "    \n",
    "# Product sentence scores    \n",
    "sent2score = {}\n",
    "for sentence in tokens:\n",
    "    for word in sentence:#nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word2count.keys():\n",
    "            if len(sentence) < 25:\n",
    "                device=sentence[0]\n",
    "                sentence_text=\" \".join(sentence)\n",
    "                if device not in sent2score:\n",
    "                    sent2score[device]={}\n",
    "                if sentence_text not in sent2score[device].keys():\n",
    "                    sent2score[device][sentence_text] = word2count[word]\n",
    "                else:\n",
    "                    sent2score[device][sentence_text] += word2count[word]\n",
    "                    \n",
    "# Gettings best 5 lines for each device\n",
    "for device, sents_scores in sent2score.items():\n",
    "    best_sentences = heapq.nlargest(10, sents_scores, key=sents_scores.get)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"\\n\",device,\":\")\n",
    "    for sent in best_sentences:\n",
    "        print(sent)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "#for sentence in best_sentences:\n",
    "#print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
